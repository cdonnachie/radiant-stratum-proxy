"""FastAPI web server for mining dashboard"""

from fastapi import FastAPI, WebSocket, Query
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.responses import FileResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from pathlib import Path
import time
import logging
import os
import asyncio
import csv
import json
from io import StringIO
from ..stratum import vardiff as _vardiff_mod
from .share_feed import get_share_feed_manager

logger = logging.getLogger("WebAPI")

app = FastAPI(title="Radiant Solo Mining Dashboard")

# Mount static files directory to serve images and other static assets
static_dir = Path(__file__).parent / "static"
app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# Store reference to state (will be set on startup)
state = None


def set_state(mining_state):
    """Set the global state reference"""
    global state
    state = mining_state


@app.get("/", response_class=HTMLResponse)
async def dashboard():
    """Serve the main dashboard HTML"""
    html_path = Path(__file__).parent / "static" / "index.html"
    if html_path.exists():
        return html_path.read_text()
    return "<h1>Dashboard HTML not found</h1>"


@app.get("/api/miners")
async def get_active_miners():
    """Get currently connected miners with live stats"""
    if not state:
        return JSONResponse({"miners": [], "total_hashrate_mhs": 0, "miner_count": 0})

    # Block-only mode: if SHARE_DIFFICULTY_DIVISOR <= 1.0, disable hashrate estimation
    try:
        divisor_val = float(os.getenv("SHARE_DIFFICULTY_DIVISOR", "1000"))
    except ValueError:
        divisor_val = 1000.0
    if divisor_val <= 1.0:
        miners_disabled = []
        now_ts = time.time()
        for session in state.all_sessions:
            worker = getattr(session, "_worker_name", "Unknown")
            miner_software = getattr(session, "_miner_software", "Unknown")
            start_time = getattr(session, "_connection_time", now_ts)
            uptime_seconds = int(now_ts - start_time)
            miners_disabled.append(
                {
                    "worker": worker,
                    "software": miner_software,
                    "assigned_difficulty": getattr(session, "_share_difficulty", None),
                    "hashrate_display": "—",
                    "hashrate_value": 0,
                    "hashrate_unit": "H/s",
                    "hashrate_instant_hs": 0,
                    "hashrate_ema_hs": 0,
                    "shares_in_window": 0,
                    "rel_error_est": 1.0,
                    "uptime_seconds": uptime_seconds,
                    "connected_at": int(start_time),
                    "hashrate_disabled": True,
                }
            )
        return JSONResponse(
            {
                "miners": miners_disabled,
                "hashrate_disabled": True,
                "total_hashrate_mhs": 0.0,
                "total_hashrate_display": "—",
                "total_instant_hs": 0.0,
                "total_ema_hs": 0.0,
                "total_instant_display": "—",
                "total_ema_display": "—",
                "total_rel_error_est": 1.0,
                "total_shares_in_window": 0,
                "miner_count": len(miners_disabled),
            }
        )

    miners = []
    current_time = time.time()
    total_instant_hs = 0.0
    total_ema_hs = 0.0
    total_shares = 0
    intervals_map = {}
    if _vardiff_mod.vardiff_manager is not None:
        try:
            intervals_map = await _vardiff_mod.vardiff_manager.all_intervals()
        except Exception:
            intervals_map = {}

    for session in state.all_sessions:
        worker = getattr(session, "_worker_name", "Unknown")
        miner_software = getattr(session, "_miner_software", "Unknown")
        assigned_diff = getattr(session, "_share_difficulty", None)

        # Get hashrate from share-based tracker with appropriate units
        from ..stratum.session import hashrate_tracker

        hashrate_display = hashrate_tracker.get_hashrate_display(worker)
        hashrate_mhs = hashrate_tracker.get_hashrate_mhs(
            worker
        )  # Keep for backward compatibility

        # Calculate connection duration
        start_time = getattr(session, "_connection_time", current_time)
        uptime_seconds = int(current_time - start_time)

        inst_hs = float(hashrate_display.get("instant", 0.0))
        ema_hs = float(hashrate_display.get("ema", inst_hs))
        shares_win = int(hashrate_display.get("shares", 0))
        total_instant_hs += inst_hs
        total_ema_hs += ema_hs
        total_shares += shares_win

        # Try VarDiff intervals first, fall back to hashrate tracker if not available
        iv = intervals_map.get(worker, {})

        # If VarDiff is disabled or worker not in VarDiff manager, use hashrate tracker intervals
        if not iv or not iv.get("blended_interval"):
            iv = hashrate_tracker.get_interval_data(worker)

        miners.append(
            {
                "worker": worker,
                "software": miner_software,
                "assigned_difficulty": assigned_diff,
                "hashrate_mhs": round(hashrate_mhs, 2),  # Legacy field
                "hashrate_display": hashrate_display[
                    "display"
                ],  # New formatted display (EMA biased)
                "hashrate_value": round(hashrate_display["value"], 2),
                "hashrate_unit": hashrate_display["unit"],
                "uptime_seconds": uptime_seconds,
                "connected_at": int(start_time),
                "hashrate_instant_hs": round(inst_hs, 2),
                "hashrate_ema_hs": round(ema_hs, 2),
                "shares_in_window": shares_win,
                "rel_error_est": round(hashrate_display.get("rel_error", 1.0), 4),
                "share_avg_interval": iv.get("avg_interval"),
                "share_ema_interval": iv.get("ema_interval"),
                "share_blended_interval": iv.get("blended_interval"),
                "target_interval": (
                    getattr(_vardiff_mod.vardiff_manager, "target", None)
                    if _vardiff_mod.vardiff_manager
                    else None
                ),
            }
        )
    # Calculate total hashrate in H/s for accurate aggregation
    total_hashrate_hs = 0
    for session in state.all_sessions:
        worker = getattr(session, "_worker_name", "Unknown")
        hashrate_display = hashrate_tracker.get_hashrate_display(worker)

        # Convert all to H/s for summation
        if hashrate_display["unit"] == "MH/s":
            total_hashrate_hs += hashrate_display["value"] * 1_000_000
        elif hashrate_display["unit"] == "KH/s":
            total_hashrate_hs += hashrate_display["value"] * 1_000
        else:  # H/s
            total_hashrate_hs += hashrate_display["value"]

    # Format total with appropriate unit
    if total_hashrate_hs >= 1_000_000:
        total_display = f"{total_hashrate_hs / 1_000_000:.2f} MH/s"
        total_mhs = total_hashrate_hs / 1_000_000
    elif total_hashrate_hs >= 1_000:
        total_display = f"{total_hashrate_hs / 1_000:.2f} KH/s"
        total_mhs = total_hashrate_hs / 1_000_000
    else:
        total_display = f"{total_hashrate_hs:.2f} H/s"
        total_mhs = total_hashrate_hs / 1_000_000

    # Helper to format dynamic units from raw H/s
    def _fmt(hs: float) -> tuple[str, float]:
        if hs >= 1_000_000_000:
            return f"{hs / 1_000_000_000:.2f} GH/s", hs / 1_000_000
        if hs >= 1_000_000:
            return f"{hs / 1_000_000:.2f} MH/s", hs / 1_000_000
        if hs >= 1_000:
            return f"{hs / 1_000:.2f} KH/s", hs / 1_000_000
        return f"{hs:.2f} H/s", hs / 1_000_000

    total_instant_display, total_instant_mhs = _fmt(total_instant_hs)
    total_ema_display, total_ema_mhs = _fmt(total_ema_hs)
    # Combined relative error (approx) using total accepted shares
    total_rel_error = 1 / (total_shares**0.5) if total_shares > 0 else 1.0

    return JSONResponse(
        {
            "miners": miners,
            "total_hashrate_mhs": round(total_mhs, 2),  # Legacy field
            "total_hashrate_display": total_display,  # Backward-compatible (EMA-like)
            "total_instant_hs": round(total_instant_hs, 2),
            "total_ema_hs": round(total_ema_hs, 2),
            "total_instant_display": total_instant_display,
            "total_ema_display": total_ema_display,
            "total_rel_error_est": round(total_rel_error, 4),
            "total_shares_in_window": total_shares,
            "miner_count": len(miners),
            "vardiff_enabled": _vardiff_mod.vardiff_manager is not None,
        }
    )


@app.get("/api/blocks")
async def get_blocks(limit: int = 100, offset: int = 0):
    """Get recent blocks found with pagination support (with in-memory fallback)"""
    try:
        from ..db.schema import get_recent_blocks

        result = await get_recent_blocks(limit, offset)
        return JSONResponse(
            {
                "blocks": result["blocks"],
                "total": result["total"],
                "source": "database",
            }
        )
    except Exception as e:
        # Fallback to in-memory tracker when database unavailable
        logger.warning(
            f"Database unavailable for blocks, using in-memory fallback: {e}"
        )
        from .block_tracker import get_block_tracker

        tracker = get_block_tracker()
        result = tracker.get_all_blocks(limit, offset)
        return JSONResponse(
            {
                "blocks": result["blocks"],
                "total": result["total"],
                "source": "memory",
            }
        )


@app.get("/api/blocks/confirmations")
async def get_block_confirmations(chain: str = None, limit: int = 50):
    """Get block confirmation status for pending/confirmed blocks"""
    try:
        from ..db.schema import get_block_confirmation_status

        blocks = await get_block_confirmation_status(chain=chain, limit=limit)

        return JSONResponse(
            {
                "blocks": blocks,
                "total": len(blocks),
                "chain": chain,
            }
        )
    except ImportError:
        return JSONResponse(
            {"error": "Database not enabled"},
            status_code=503,
        )
    except Exception as e:
        logger.error("Failed to get block confirmations: %s", e)
        return JSONResponse(
            {"error": str(e)},
            status_code=500,
        )


@app.get("/api/blocks/pending")
async def get_pending_blocks(chain: str = None):
    """Get pending blocks awaiting confirmation"""
    try:
        from ..db.schema import get_pending_blocks

        blocks = await get_pending_blocks(chain=chain)

        return JSONResponse(
            {
                "blocks": blocks,
                "total": len(blocks),
                "chain": chain,
            }
        )
    except ImportError:
        return JSONResponse(
            {"error": "Database not enabled"},
            status_code=503,
        )
    except Exception as e:
        logger.error("Failed to get pending blocks: %s", e)
        return JSONResponse(
            {"error": str(e)},
            status_code=500,
        )


@app.get("/api/blocks/{chain}")
async def get_chain_blocks(chain: str, limit: int = 10, offset: int = 0):
    """Get recent blocks for a specific chain (RXD) with pagination"""
    try:
        from ..db.schema import get_blocks_by_chain

        result = await get_blocks_by_chain(chain.upper(), limit, offset)
        return JSONResponse(
            {
                "blocks": result["blocks"],
                "total": result["total"],
                "chain": chain.upper(),
                "source": "database",
            }
        )
    except Exception as e:
        # Fallback to in-memory tracker when database unavailable
        logger.warning(
            f"Database unavailable for {chain} blocks, using in-memory fallback: {e}"
        )
        from .block_tracker import get_block_tracker

        tracker = get_block_tracker()
        result = tracker.get_blocks_by_chain(chain.upper(), limit, offset)
        return JSONResponse(
            {
                "blocks": result["blocks"],
                "total": result["total"],
                "chain": chain.upper(),
                "source": "memory",
            }
        )


@app.get("/api/best-shares")
async def get_best_shares():
    """Get best shares unified for merged mining"""
    try:
        from ..db.schema import get_unified_best_shares

        shares = await get_unified_best_shares(limit=10)

        return JSONResponse({"shares": shares})
    except Exception as e:
        logger.error(f"Error getting best shares: {e}")
        return JSONResponse({"shares": []})


@app.get("/api/best-shares/{chain}")
async def get_best_shares_by_chain(chain: str, limit: int = 10):
    """Get best shares for a specific chain"""
    try:
        from ..db.schema import get_best_shares

        shares = await get_best_shares(chain.upper(), limit=limit)
        return JSONResponse({"shares": shares, "chain": chain.upper()})
    except Exception as e:
        logger.error(f"Error getting best shares for {chain}: {e}")
        return JSONResponse({"shares": [], "chain": chain.upper()})


@app.get("/api/difficulty-history/{chain}")
async def get_difficulty_history(chain: str, hours: int = 24):
    """Get difficulty history for a specific chain over the last N hours"""
    try:
        from ..db.schema import get_difficulty_history

        history = await get_difficulty_history(chain.upper(), hours)
        return JSONResponse({"chain": chain.upper(), "hours": hours, "data": history})
    except Exception as e:
        logger.error(f"Error getting difficulty history for {chain}: {e}")
        return JSONResponse({"chain": chain.upper(), "hours": hours, "data": []})


@app.get("/api/hashrate-history")
async def get_hashrate_history(hours: int = 24):
    """Get hashrate history for the last N hours"""
    try:
        from ..db.schema import get_hashrate_history

        history = await get_hashrate_history(hours)
        return JSONResponse({"hours": hours, "data": history})
    except Exception as e:
        logger.error(f"Error getting hashrate history: {e}")
        return JSONResponse({"hours": hours, "data": []})


@app.get("/api/stats")
async def get_stats(hours: int = 24):
    """Get summary statistics"""
    from ..db.schema import get_stats_summary

    stats = await get_stats_summary(hours)

    # Add current difficulty and target info
    if state:
        rxd_difficulty = 0
        if state.target:
            try:
                from ..consensus.targets import target_to_diff1

                rxd_target_int = int(state.target, 16)
                rxd_difficulty = target_to_diff1(rxd_target_int)
            except Exception as e:
                logger.debug(f"Failed to compute RXD difficulty from target: {e}")

        if rxd_difficulty == 0:
            try:
                import aiohttp
                from ..config import Settings

                settings = Settings()

                async with aiohttp.ClientSession() as session:
                    payload = {
                        "jsonrpc": "1.0",
                        "id": "get_difficulty",
                        "method": "getblockchaininfo",
                        "params": [],
                    }
                    rxd_url = f"http://{settings.rpcuser}:{settings.rpcpass}@{settings.rpcip}:{settings.rpcport}"
                    async with session.post(
                        rxd_url,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=2),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if "result" in data and data["result"]:
                                rxd_difficulty = data["result"].get("difficulty", 0)
            except Exception as e:
                logger.debug(f"Could not fetch RXD difficulty from daemon: {e}")

        stats["current_rxd_difficulty"] = rxd_difficulty
        stats["current_height_rxd"] = state.height

        # Calculate Time-To-Find (TTF) estimates
        # Formula: TTF = (network_difficulty * 2^32) / hashrate_in_hs
        # Get total EMA hashrate from all miners
        from ..stratum.session import hashrate_tracker

        total_ema_hs = 0.0
        for session in state.all_sessions:
            worker = getattr(session, "_worker_name", "Unknown")
            hashrate_display = hashrate_tracker.get_hashrate_display(worker)
            ema_hs = float(hashrate_display.get("ema", 0.0))
            total_ema_hs += ema_hs

        # Initialize all TTF fields to None
        stats["ttf_rxd_seconds"] = None
        stats["ttf_rxd_12h_seconds"] = None
        stats["ttf_rxd_24h_seconds"] = None

        # Calculate TTF Now (using current EMA hashrate and current difficulty)
        if total_ema_hs > 0:
            rxd_diff = stats.get("current_rxd_difficulty", 0)

            if rxd_diff > 0:
                # TTF = (difficulty * 2^32) / hashrate
                stats["ttf_rxd_seconds"] = (rxd_diff * (2**32)) / total_ema_hs

        # Calculate TTF based on historical averages if database is enabled
        db_enabled = os.getenv("ENABLE_DATABASE", "false").lower() == "true"
        if db_enabled:
            try:
                from ..db.schema import get_hashrate_history, get_difficulty_history

                # Get 12-hour and 24-hour averages
                hashrate_12h_data = await get_hashrate_history(hours=12)
                hashrate_24h_data = await get_hashrate_history(hours=24)
                difficulty_12h_rxd = await get_difficulty_history("RXD", hours=12)
                difficulty_24h_rxd = await get_difficulty_history("RXD", hours=24)

                # Calculate average hashrates
                avg_hashrate_12h = 0.0
                if hashrate_12h_data:
                    hashrates_12h = [h["hashrate_hs"] for h in hashrate_12h_data]
                    avg_hashrate_12h = (
                        sum(hashrates_12h) / len(hashrates_12h)
                        if hashrates_12h
                        else 0.0
                    )

                avg_hashrate_24h = 0.0
                if hashrate_24h_data:
                    hashrates_24h = [h["hashrate_hs"] for h in hashrate_24h_data]
                    avg_hashrate_24h = (
                        sum(hashrates_24h) / len(hashrates_24h)
                        if hashrates_24h
                        else 0.0
                    )

                # Calculate average difficulties for RXD
                avg_difficulty_12h_rxd = 0.0
                if difficulty_12h_rxd:
                    diffs = [d["difficulty"] for d in difficulty_12h_rxd]
                    avg_difficulty_12h_rxd = sum(diffs) / len(diffs) if diffs else 0.0

                avg_difficulty_24h_rxd = 0.0
                if difficulty_24h_rxd:
                    diffs = [d["difficulty"] for d in difficulty_24h_rxd]
                    avg_difficulty_24h_rxd = sum(diffs) / len(diffs) if diffs else 0.0

                # Calculate TTF 12h (average hashrate from last 12h vs current difficulty)
                if avg_hashrate_12h > 0:
                    if avg_difficulty_12h_rxd > 0:
                        stats["ttf_rxd_12h_seconds"] = (
                            avg_difficulty_12h_rxd * (2**32)
                        ) / avg_hashrate_12h

                # Calculate TTF 24h (average hashrate from last 24h vs current difficulty)
                if avg_hashrate_24h > 0:
                    if avg_difficulty_24h_rxd > 0:
                        stats["ttf_rxd_24h_seconds"] = (
                            avg_difficulty_24h_rxd * (2**32)
                        ) / avg_hashrate_24h

            except Exception as e:
                logger.debug(f"Could not calculate historical TTF estimates: {e}")

    return JSONResponse(stats)


@app.get("/api/payouts")
async def get_payout_info():
    """Get payout address information"""
    import os

    payout_info = {
        "rxd_address": None,
        "rxd_source": "not_set",
    }

    # Get RXD address from first connected miner (if available)
    if state and hasattr(state, "pub_h160") and state.pub_h160:
        try:
            # Reconstruct the address from the stored pub_h160
            import base58

            version = 111 if getattr(state, "testnet", False) else 0
            rxd_address = base58.b58encode_check(
                bytes([version]) + state.pub_h160
            ).decode()

            payout_info["rxd_address"] = rxd_address
            payout_info["rxd_source"] = "first_miner"
        except Exception as e:
            logger.debug("Failed to reconstruct RXD address: %s", e)
            # Fallback: show partial info
            payout_info["rxd_address"] = (
                f"Set by first miner (pub_h160: {state.pub_h160.hex()[:20]}...)"
            )
            payout_info["rxd_source"] = "first_miner"

    return JSONResponse(payout_info)


@app.get("/api/vardiff_state")
async def get_vardiff_state():
    """Inspect current VarDiff manager state (if enabled)."""
    manager = _vardiff_mod.vardiff_manager
    if manager is None:
        return JSONResponse({"enabled": False})
    try:
        return JSONResponse({"enabled": True, **manager.export_state()})
    except Exception as e:
        logger.error("Error exporting vardiff state: %s", e, exc_info=True)
        return JSONResponse(
            {"enabled": True, "error": "Failed to retrieve state"}, status_code=500
        )


@app.get("/favicon.ico")
async def favicon():
    """Serve a favicon (reuse radiant logo) to avoid 404 noise."""
    logo_path = static_dir / "radiant-logo.png"
    if logo_path.exists():
        return FileResponse(str(logo_path))
    # Fallback: 1x1 transparent GIF bytes
    from fastapi import Response

    transparent_gif = (
        b"GIF89a\x01\x00\x01\x00\x80\x00\x00\x00\x00\x00"
        b"\xff\xff\xff!\xf9\x04\x01\x00\x00\x00\x00,\x00"
        b"\x00\x00\x00\x01\x00\x01\x00\x00\x02\x02D\x01\x00;"
    )
    return Response(content=transparent_gif, media_type="image/gif")


@app.post("/api/flush_hashrate")
async def flush_hashrate():
    """Clear in-memory hashrate tracking (5m window + EMA) for a fresh start."""
    from ..stratum.session import hashrate_tracker

    cleared_workers = len(hashrate_tracker.worker_shares)
    hashrate_tracker.worker_shares.clear()
    hashrate_tracker.worker_ema.clear()
    return JSONResponse(
        {
            "status": "flushed",
            "cleared_workers": cleared_workers,
            "timestamp": int(time.time()),
        }
    )


@app.post("/api/clear_best_shares")
async def clear_best_shares():
    """Clear all best shares from database and start fresh tracking."""
    try:
        import aiosqlite
        from ..db.schema import DB_PATH

        async with aiosqlite.connect(DB_PATH) as db:
            # Delete all best shares
            await db.execute("DELETE FROM best_shares")
            deleted_count = db.total_changes
            await db.commit()

        return JSONResponse(
            {
                "status": "cleared",
                "deleted_count": deleted_count,
                "timestamp": int(time.time()),
            }
        )
    except Exception as e:
        logger.error("Error clearing best shares: %s", e)
        return JSONResponse(
            {"status": "error", "message": "Failed to clear shares"}, status_code=500
        )


@app.get("/api/shares/summary")
async def get_share_stats(worker: str = None, minutes: int = 10):
    """Get recent share statistics for debugging hashrate calculation"""
    try:
        from ..db.schema import get_recent_share_stats

        stats = await get_recent_share_stats(worker=worker, minutes=minutes)

        # Calculate some summary info
        if stats:
            total_accepted = sum(s.get("shares_accepted", 0) for s in stats)
            total_rejected = sum(s.get("shares_rejected", 0) for s in stats)
            avg_difficulties = [
                s.get("avg_difficulty", 0) for s in stats if s.get("avg_difficulty")
            ]
            avg_difficulty = (
                sum(avg_difficulties) / len(avg_difficulties) if avg_difficulties else 0
            )

            summary = {
                "total_accepted": total_accepted,
                "total_rejected": total_rejected,
                "acceptance_rate": (
                    (total_accepted / (total_accepted + total_rejected) * 100)
                    if (total_accepted + total_rejected) > 0
                    else None
                ),
                "average_difficulty": avg_difficulty,
                "time_span_minutes": minutes,
            }
        else:
            summary = {
                "total_accepted": 0,
                "total_rejected": 0,
                "acceptance_rate": None,
                "average_difficulty": 0,
                "time_span_minutes": minutes,
            }

        return JSONResponse(
            {"stats": stats, "summary": summary, "worker_filter": worker}
        )

    except ImportError:
        return JSONResponse({"error": "Database not enabled"}, status_code=503)
    except Exception as e:
        logger.error("Error retrieving share stats: %s", e)
        return JSONResponse(
            {"error": "Failed to retrieve share statistics"}, status_code=500
        )


@app.post("/api/cleanup")
async def manual_cleanup():
    """Manually trigger database cleanup"""
    try:
        from ..db.schema import cleanup_old_data

        await cleanup_old_data()
        return JSONResponse(
            {"status": "cleanup completed", "timestamp": int(time.time())}
        )

    except ImportError:
        return JSONResponse({"error": "Database not enabled"}, status_code=503)
    except Exception as e:
        logger.error("Error during cleanup: %s", e)
        return JSONResponse({"error": "Failed to complete cleanup"}, status_code=500)


@app.get("/api/health")
async def health_check():
    """Simple health check endpoint"""
    return JSONResponse(
        {"status": "ok", "timestamp": int(time.time()), "database": "connected"}
    )


@app.get("/api/system/config")
async def get_system_config():
    """Get current system configuration and enabled features"""
    config_data = {
        "vardiff": {
            "enabled": _vardiff_mod.vardiff_manager is not None,
            "target_interval": float(os.getenv("VARDIFF_TARGET_INTERVAL", "15.0")),
            "min_difficulty": float(os.getenv("VARDIFF_MIN_DIFFICULTY", "0.00001")),
            "max_difficulty": float(os.getenv("VARDIFF_MAX_DIFFICULTY", "0.1")),
        },
        "zmq": {
            "enabled": os.getenv("ENABLE_ZMQ", "true").lower() == "true",
            "rxd_endpoint": os.getenv("RXD_ZMQ_ENDPOINT", "tcp://radiant:28332"),
        },
        "notifications": {
            "discord": bool(os.getenv("DISCORD_WEBHOOK_URL", "").strip()),
            "telegram": bool(
                os.getenv("TELEGRAM_BOT_TOKEN", "").strip()
                and os.getenv("TELEGRAM_CHAT_ID", "").strip()
            ),
        },
        "database": {
            "enabled": os.getenv("ENABLE_DATABASE", "false").lower() == "true",
        },
        "stratum": {
            "port": int(os.getenv("STRATUM_PORT", "54321")),
            "share_divisor": float(os.getenv("SHARE_DIFFICULTY_DIVISOR", "1000")),
        },
    }
    return JSONResponse(config_data)


@app.get("/api/daemon-status")
async def get_daemon_status():
    """Get blockchain daemon status for RXD node"""
    import aiohttp
    import asyncio
    from ..config import Settings

    settings = Settings()

    async def get_blockchain_info(url: str, chain: str):
        """Get blockchain info from a node"""
        try:
            async with aiohttp.ClientSession() as session:
                payload = {
                    "jsonrpc": "1.0",
                    "id": "daemon_status",
                    "method": "getblockchaininfo",
                    "params": [],
                }
                async with session.post(
                    url, json=payload, timeout=aiohttp.ClientTimeout(total=5)
                ) as resp:
                    if resp.status == 200:
                        data = await resp.json()
                        if "result" in data and data["result"]:
                            result = data["result"]
                            # Try to get network info for connections
                            net_payload = {
                                "jsonrpc": "1.0",
                                "id": "daemon_status",
                                "method": "getnetworkinfo",
                                "params": [],
                            }
                            try:
                                async with session.post(
                                    url,
                                    json=net_payload,
                                    timeout=aiohttp.ClientTimeout(total=3),
                                ) as net_resp:
                                    if net_resp.status == 200:
                                        net_data = await net_resp.json()
                                        net_result = net_data.get("result", {})
                                        connections = net_result.get("connections", 0)
                                        version = net_result.get(
                                            "subversion", "Unknown"
                                        ).strip("/")
                                    else:
                                        connections = "—"
                                        version = "—"
                            except:
                                connections = "—"
                                version = "—"

                            return {
                                "status": "Connected",
                                "sync": (
                                    f"{result.get('verificationprogress', 0) * 100:.1f}%"
                                    if result.get("verificationprogress")
                                    else "—"
                                ),
                                "connections": connections,
                                "version": version,
                                "blocks": result.get("blocks", 0),
                                "difficulty": result.get("difficulty", 0),
                                "chain": result.get("chain", "main"),
                                "synced": result.get("verificationprogress", 0)
                                >= 0.999,
                            }
                    return {
                        "status": "Error",
                        "sync": "—",
                        "connections": "—",
                        "version": "—",
                        "blocks": 0,
                        "difficulty": 0,
                        "chain": "—",
                        "synced": False,
                        "error": f"HTTP {resp.status}",
                    }
        except asyncio.TimeoutError:
            return {
                "status": "Timeout",
                "sync": "—",
                "connections": "—",
                "version": "—",
                "blocks": 0,
                "difficulty": 0,
                "chain": "—",
                "synced": False,
                "error": "Connection timeout",
            }
        except Exception as e:
            logger.error(f"Error getting {chain} blockchain info: {e}")
            return {
                "status": "Offline",
                "sync": "—",
                "connections": "—",
                "version": "—",
                "blocks": 0,
                "difficulty": 0,
                "chain": "—",
                "synced": False,
                "error": str(e),
            }

    # Build RPC URL from settings
    rxd_url = f"http://{settings.rpcuser}:{settings.rpcpass}@{settings.rpcip}:{settings.rpcport}"

    # Get status from RXD node
    result = await get_blockchain_info(rxd_url, "RXD")

    response = {
        "RXD": result if not isinstance(result, Exception) else {
            "status": "Error",
            "sync": "—",
            "connections": "—",
            "version": "—",
            "blocks": 0,
            "difficulty": 0,
            "chain": "—",
            "synced": False,
        },
    }

    return JSONResponse(response)


@app.get("/api/miners/connected")
async def get_connected_miners_paginated(page: int = 1, limit: int = 20):
    """Get connected miners with pagination"""
    try:
        from ..db.schema import get_connected_miners

        if page < 1:
            page = 1
        offset = (page - 1) * limit

        result = await get_connected_miners(offset=offset, limit=limit)
        return JSONResponse(
            {
                "miners": result["miners"],
                "total": result["total"],
                "page": page,
                "limit": limit,
                "pages": (result["total"] + limit - 1) // limit,
            }
        )
    except Exception as e:
        logger.error(f"Error getting connected miners: {e}")
        return JSONResponse(
            {"miners": [], "total": 0, "page": page, "limit": limit, "pages": 0}
        )


@app.get("/api/miners/disconnected")
async def get_disconnected_miners_paginated(
    hours: int = 24, page: int = 1, limit: int = 20
):
    """Get recently disconnected miners with pagination"""
    try:
        from ..db.schema import get_disconnected_miners

        if page < 1:
            page = 1
        offset = (page - 1) * limit

        result = await get_disconnected_miners(hours=hours, offset=offset, limit=limit)
        return JSONResponse(
            {
                "miners": result["miners"],
                "total": result["total"],
                "page": page,
                "limit": limit,
                "pages": (result["total"] + limit - 1) // limit,
                "hours": hours,
            }
        )
    except Exception as e:
        logger.error(f"Error getting disconnected miners: {e}")
        return JSONResponse(
            {
                "miners": [],
                "total": 0,
                "page": page,
                "limit": limit,
                "pages": 0,
                "hours": hours,
            }
        )


@app.post("/api/miners/{worker_name}/clear")
async def clear_miner_record(worker_name: str):
    """Delete a miner session record"""
    try:
        from ..db.schema import delete_miner_session

        await delete_miner_session(worker_name)
        return JSONResponse({"status": "success", "worker_name": worker_name})
    except Exception as e:
        logger.error("Error deleting miner record %s: %s", worker_name, e)
        return JSONResponse(
            {"status": "error", "message": "Failed to delete record"}, status_code=500
        )


@app.get("/api/earnings")
async def get_earnings_estimate():
    """Get estimated daily/weekly earnings based on current hashrate and difficulty for Radiant"""
    try:
        from ..utils.earnings import EarningsCalculator
        from ..utils.price_tracker import get_price_tracker
        from ..consensus.targets import target_to_diff1
        from ..config import Settings
        import aiohttp

        if not state:
            return JSONResponse(
                {"status": "error", "message": "State not initialized"},
                status_code=500,
            )

        # Get current prices
        price_tracker = get_price_tracker()
        prices = await price_tracker.get_current_prices()

        # Calculate total hashrate from all connected miners
        total_hashrate_hs = 0.0
        from ..stratum.session import hashrate_tracker

        for session in state.all_sessions:
            worker = getattr(session, "_worker_name", "Unknown")
            hashrate_display = hashrate_tracker.get_hashrate_display(worker)
            ema_hs = float(hashrate_display.get("ema", 0.0))
            total_hashrate_hs += ema_hs

        # Get RXD difficulty from target or RPC
        rxd_difficulty = 0.0
        if state.rxd_original_target:
            try:
                rxd_target_int = int(state.rxd_original_target, 16)
                rxd_difficulty = target_to_diff1(rxd_target_int)
            except Exception as e:
                logger.debug(f"Failed to compute RXD difficulty from target: {e}")

        if rxd_difficulty == 0.0:
            try:
                settings = Settings()
                async with aiohttp.ClientSession() as session:
                    payload = {
                        "jsonrpc": "1.0",
                        "id": "get_difficulty",
                        "method": "getblockchaininfo",
                        "params": [],
                    }
                    rxd_url = f"http://{settings.rpcuser}:{settings.rpcpass}@{settings.rpcip}:{settings.rpcport}"
                    async with session.post(
                        rxd_url,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=2),
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            if "result" in data and data["result"]:
                                rxd_difficulty = float(
                                    data["result"].get("difficulty", 0)
                                )
            except Exception as e:
                logger.debug(f"Could not fetch RXD difficulty from daemon: {e}")

        # Ensure minimum difficulty of 1.0 to avoid division by zero
        if rxd_difficulty <= 0:
            rxd_difficulty = 1.0

        # Get block reward from daemon (via getblocktemplate)
        # Radiant uses 8 decimal places (like Bitcoin)
        # Block reward is cached with 1-hour TTL to avoid excessive RPC calls
        block_rewards = await price_tracker.get_block_rewards()
        rxd_block_reward = block_rewards.get("rxd_block_reward", 25000.0)

        # Log calculation inputs
        logger.debug(
            f"Earnings calculation: hashrate={total_hashrate_hs}H/s, "
            f"rxd_diff={rxd_difficulty}, rxd_reward={rxd_block_reward}, "
            f"rxd_price=${prices.get('rxd_price_usd')}"
        )

        # Calculate earnings with actual block rewards - using NOW (current EMA hashrate)
        earnings_now = EarningsCalculator.calculate_daily_earnings(
            hashrate_hs=total_hashrate_hs,
            rxd_difficulty=rxd_difficulty,
            rxd_price_btc=prices.get("rxd_price"),
            rxd_price_usd=prices.get("rxd_price_usd"),
            rxd_block_reward=rxd_block_reward,
        )

        # Use 24h average if database enabled, otherwise use NOW as default
        earnings = earnings_now.copy()
        db_enabled = os.getenv("ENABLE_DATABASE", "false").lower() == "true"
        earnings_12h = None
        earnings_24h = None

        if db_enabled:
            try:
                from ..db.schema import get_hashrate_history, get_difficulty_history

                # Get 12-hour and 24-hour averages
                hashrate_12h_data = await get_hashrate_history(hours=12)
                hashrate_24h_data = await get_hashrate_history(hours=24)
                difficulty_12h = await get_difficulty_history("RXD", hours=12)
                difficulty_24h = await get_difficulty_history("RXD", hours=24)

                # Calculate average hashrates
                avg_hashrate_12h = 0.0
                if hashrate_12h_data:
                    hashrates_12h = [h["hashrate_hs"] for h in hashrate_12h_data]
                    avg_hashrate_12h = (
                        sum(hashrates_12h) / len(hashrates_12h)
                        if hashrates_12h
                        else 0.0
                    )

                avg_hashrate_24h = 0.0
                if hashrate_24h_data:
                    hashrates_24h = [h["hashrate_hs"] for h in hashrate_24h_data]
                    avg_hashrate_24h = (
                        sum(hashrates_24h) / len(hashrates_24h)
                        if hashrates_24h
                        else 0.0
                    )

                # Calculate average difficulties
                avg_difficulty_12h = 0.0
                if difficulty_12h:
                    diffs = [d["difficulty"] for d in difficulty_12h]
                    avg_difficulty_12h = sum(diffs) / len(diffs) if diffs else 0.0

                avg_difficulty_24h = 0.0
                if difficulty_24h:
                    diffs = [d["difficulty"] for d in difficulty_24h]
                    avg_difficulty_24h = sum(diffs) / len(diffs) if diffs else 0.0

                # Calculate earnings for 12h average
                if avg_hashrate_12h > 0:
                    earnings_12h = EarningsCalculator.calculate_daily_earnings(
                        hashrate_hs=avg_hashrate_12h,
                        rxd_difficulty=avg_difficulty_12h if avg_difficulty_12h > 0 else rxd_difficulty,
                        rxd_price_btc=prices.get("rxd_price"),
                        rxd_price_usd=prices.get("rxd_price_usd"),
                        rxd_block_reward=rxd_block_reward,
                    )

                # Calculate earnings for 24h average
                if avg_hashrate_24h > 0:
                    earnings_24h = EarningsCalculator.calculate_daily_earnings(
                        hashrate_hs=avg_hashrate_24h,
                        rxd_difficulty=avg_difficulty_24h if avg_difficulty_24h > 0 else rxd_difficulty,
                        rxd_price_btc=prices.get("rxd_price"),
                        rxd_price_usd=prices.get("rxd_price_usd"),
                        rxd_block_reward=rxd_block_reward,
                    )
                    # Use 24h as the default when available
                    earnings = earnings_24h.copy()

            except Exception as e:
                logger.debug(f"Could not calculate historical earnings: {e}")

        # Log results
        logger.debug(
            f"Earnings result: rxd_coins_per_day={earnings.get('rxd_coins_per_day')}, "
            f"rxd_usd_per_day={earnings.get('rxd_usd_per_day')}"
        )

        # Add price information and current metrics
        earnings["prices"] = {
            "rxd_price_btc": prices.get("rxd_price"),
            "rxd_price_usd": prices.get("rxd_price_usd"),
            "price_timestamp": prices.get("timestamp"),
        }
        earnings["current_metrics"] = {
            "total_hashrate_hs": round(total_hashrate_hs, 2),
            "rxd_difficulty": rxd_difficulty,
        }

        # Add scenario-based earnings for hover card
        earnings["earnings_scenarios"] = {
            "now": earnings_now if earnings_now else None,
            "avg_12h": earnings_12h if earnings_12h else None,
            "avg_24h": earnings_24h if earnings_24h else None,
        }
        earnings["has_historical_data"] = db_enabled and (earnings_12h or earnings_24h)

        # Calculate actual earnings from blocks found (dynamic date range)
        if db_enabled:
            try:
                from ..db.schema import get_stats_summary, DB_PATH
                import aiosqlite

                # Get 7-day block data first
                stats_7d = await get_stats_summary(hours=168)  # 7 days = 168 hours
                blocks_found_7d = stats_7d.get("blocks", {})
                rxd_blocks_7d = blocks_found_7d.get("RXD", 0)

                # Query database directly to find actual date range of blocks
                actual_days = 1
                try:
                    async with aiosqlite.connect(DB_PATH) as db:
                        # Get oldest and newest block timestamps
                        cursor = await db.execute(
                            "SELECT MIN(timestamp) as oldest, MAX(timestamp) as newest FROM blocks"
                        )
                        row = await cursor.fetchone()

                        if row and row[0] and row[1]:
                            # Calculate actual days in database
                            time_span_seconds = row[1] - row[0]
                            actual_days = max(
                                1, time_span_seconds / 86400
                            )  # at least 1 day
                except Exception as e:
                    logger.debug(f"Could not determine block date range: {e}")
                    actual_days = 1

                # Calculate actual earnings from found blocks
                rxd_actual_coins_7d = rxd_blocks_7d * rxd_block_reward

                # Calculate daily average from actual days available
                rxd_daily_avg = rxd_actual_coins_7d / actual_days

                rxd_price_usd = prices.get("rxd_price_usd")

                # Actual totals for available period
                rxd_actual_usd_7d = (
                    rxd_actual_coins_7d * rxd_price_usd if rxd_price_usd else 0
                )

                # Daily average from actual days
                rxd_daily_avg_usd = (
                    rxd_daily_avg * rxd_price_usd if rxd_price_usd else 0
                )

                # Weekly projection (if less than 7 days, project to 7 days)
                rxd_weekly_projection = rxd_daily_avg_usd * 7

                earnings["actual_7d_earnings"] = {
                    "rxd_blocks_7d": rxd_blocks_7d,
                    "actual_days": round(actual_days, 1),
                    "rxd_coins_7d": round(rxd_actual_coins_7d, 8),
                    "rxd_usd_per_day_avg": round(rxd_daily_avg_usd, 2),
                    "rxd_usd_per_week": round(rxd_weekly_projection, 2),
                }
            except Exception as e:
                logger.debug(f"Could not calculate actual earnings: {e}")
                earnings["actual_7d_earnings"] = None

        return JSONResponse(earnings)

    except Exception as e:
        logger.error("Error calculating earnings: %s", e, exc_info=True)
        return JSONResponse(
            {"status": "error", "message": "Failed to calculate earnings"},
            status_code=500,
        )


@app.get("/shares")
async def shares_page() -> HTMLResponse:
    """Serve the shares live feed page"""
    html_path = Path(__file__).parent / "static" / "shares.html"
    if html_path.exists():
        return HTMLResponse(html_path.read_text())
    return HTMLResponse("<h1>Shares page not found</h1>")


@app.get("/api/shares")
async def get_shares(
    limit: int = Query(100, ge=1, le=500),
    offset: int = Query(0, ge=0),
    worker: str = Query(None),
    accepted_only: bool = Query(False),
    blocks_only: bool = Query(False),
):
    """Get recent shares with optional filtering"""
    feed_manager = get_share_feed_manager()
    result = await feed_manager.get_shares(
        limit=limit,
        offset=offset,
        worker=worker,
        accepted_only=accepted_only,
        blocks_only=blocks_only,
    )
    return JSONResponse(result)


@app.get("/api/shares/stats")
async def get_shares_stats():
    """Get share statistics"""
    feed_manager = get_share_feed_manager()
    stats = await feed_manager.get_statistics()

    # Add connected workers count from Stratum sessions
    if state:
        stats["connected_workers"] = len(state.all_sessions)
    else:
        stats["connected_workers"] = 0

    return JSONResponse(stats)


@app.get("/api/blocks/export/json")
async def export_blocks_json(chain: str = None):
    """Export all blocks as JSON file"""
    try:
        from ..db.schema import get_recent_blocks
        from datetime import datetime

        # Get all blocks (use a high limit)
        result = await get_recent_blocks(limit=999999, offset=0)
        blocks = result["blocks"]

        # Filter by chain if specified
        if chain:
            chain = chain.upper()
            blocks = [b for b in blocks if b.get("chain") == chain]

        # Convert Row objects to dicts if needed
        blocks_list = []
        for block in blocks:
            if isinstance(block, dict):
                blocks_list.append(block)
            else:
                # Convert Row or other object to dict
                blocks_list.append(dict(block))

        # Format timestamp to ISO datetime for better readability
        for block in blocks_list:
            if "timestamp" in block and block["timestamp"]:
                try:
                    block["timestamp_iso"] = datetime.utcfromtimestamp(
                        block["timestamp"]
                    ).isoformat()
                except (TypeError, ValueError):
                    block["timestamp_iso"] = str(block["timestamp"])

        # Create JSON string
        json_data = json.dumps(blocks_list, indent=2, default=str)

        # Determine filename
        chain_suffix = f"_{chain}" if chain else "_all"
        filename = (
            f"blocks_{datetime.now().strftime('%Y%m%d_%H%M%S')}{chain_suffix}.json"
        )

        # Return as file download
        return StreamingResponse(
            iter([json_data]),
            media_type="application/json",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )
    except Exception as e:
        logger.error(f"Failed to export blocks as JSON: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/api/blocks/export/csv")
async def export_blocks_csv(chain: str = None):
    """Export all blocks as CSV file"""
    try:
        from ..db.schema import get_recent_blocks
        from datetime import datetime

        # Get all blocks (use a high limit)
        result = await get_recent_blocks(limit=999999, offset=0)
        blocks = result["blocks"]

        # Filter by chain if specified
        if chain:
            chain = chain.upper()
            blocks = [b for b in blocks if b.get("chain") == chain]

        # Create CSV
        output = StringIO()

        if blocks:
            # Convert Row objects to dicts if needed
            blocks_list = []
            for block in blocks:
                if isinstance(block, dict):
                    blocks_list.append(block)
                else:
                    # Convert Row or other object to dict
                    blocks_list.append(dict(block))

            # Get all field names from first block
            fieldnames = list(blocks_list[0].keys())

            # Convert timestamps to ISO format for readability
            for block in blocks_list:
                if "timestamp" in block and block["timestamp"]:
                    try:
                        block["timestamp_iso"] = datetime.utcfromtimestamp(
                            block["timestamp"]
                        ).isoformat()
                    except (TypeError, ValueError):
                        block["timestamp_iso"] = str(block["timestamp"])

            # Add timestamp_iso to fieldnames if it was added
            if "timestamp_iso" not in fieldnames:
                fieldnames.append("timestamp_iso")

            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()

            # Write all rows
            for block in blocks_list:
                writer.writerow(block)
        else:
            # Empty export
            writer = csv.writer(output)
            writer.writerow(
                [
                    "chain",
                    "height",
                    "block_hash",
                    "worker",
                    "difficulty",
                    "timestamp",
                    "accepted",
                ]
            )

        csv_data = output.getvalue()

        # Determine filename
        chain_suffix = f"_{chain}" if chain else "_all"
        filename = (
            f"blocks_{datetime.now().strftime('%Y%m%d_%H%M%S')}{chain_suffix}.csv"
        )

        # Return as file download
        return StreamingResponse(
            iter([csv_data]),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename={filename}"},
        )
    except Exception as e:
        logger.error(f"Failed to export blocks as CSV: {e}", exc_info=True)
        return JSONResponse({"error": str(e)}, status_code=500)


@app.websocket("/ws/shares")
async def websocket_shares(websocket: WebSocket):
    """WebSocket endpoint for real-time share feed streaming"""
    logger.info("WebSocket connection attempt from %s", websocket.client)
    try:
        await websocket.accept()
        logger.info("WebSocket client accepted: %s", websocket.client)
    except Exception as e:
        logger.error(
            "Failed to accept WebSocket connection from %s: %s",
            websocket.client,
            e,
            exc_info=True,
        )
        return

    feed_manager = get_share_feed_manager()

    # Create a queue for this client with larger buffer for slower connections
    try:
        logger.debug("Creating queue for WebSocket client: %s", websocket.client)
        client_queue: asyncio.Queue = asyncio.Queue(
            maxsize=feed_manager._client_queue_size
        )
        logger.debug("Queue created, registering with feed manager")
        await feed_manager.register_client(client_queue)
        logger.info("WebSocket client registered with queue: %s", websocket.client)
    except Exception as e:
        logger.error(
            "Failed to register WebSocket client %s: %s",
            websocket.client,
            e,
            exc_info=True,
        )
        try:
            await websocket.close(code=1011)
        except Exception:
            pass
        return

    try:
        while True:
            try:
                # Wait for message from queue with timeout for keepalive
                message = await asyncio.wait_for(client_queue.get(), timeout=30)
                try:
                    await websocket.send_text(message)
                except Exception as send_error:
                    logger.debug("Failed to send message: %s", send_error)
                    break
            except asyncio.TimeoutError:
                # Send keepalive ping to prevent client timeout
                try:
                    await websocket.send_json({"type": "ping"})
                except Exception as ping_error:
                    logger.debug("Failed to send ping: %s", ping_error)
                    break
    except Exception as e:
        logger.debug("WebSocket loop error: %s", e)
    finally:
        await feed_manager.unregister_client(client_queue)
        try:
            await websocket.close()
        except Exception:
            pass
        logger.debug("WebSocket client disconnected")
